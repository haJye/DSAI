{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install spmf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGF8wAJY3BHO",
        "outputId": "d121b716-5c69-495e-b52d-e9680229b2a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spmf\n",
            "  Downloading spmf-1.4-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: spmf\n",
            "Successfully installed spmf-1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dc0S5Ga2rfJ",
        "outputId": "a27b549f-1996-4024-81fb-b7e20f1e4829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to classifier_results.txt\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "#from spmf import Spmf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/car.data\", header=None)\n",
        "df.columns = ['buying', 'maintenance', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
        "\n",
        "# Preprocessing: Convert categorical variables to numerical\n",
        "df = pd.get_dummies(df, columns=['buying', 'maintenance', 'doors', 'persons', 'lug_boot', 'safety'])\n",
        "\n",
        "# Split data into features and target\n",
        "X = df.drop(columns=['class'])\n",
        "y = df['class']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define classifiers\n",
        "classifiers = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB()\n",
        "}\n",
        "\n",
        "# Train and evaluate classifiers\n",
        "results = {}\n",
        "for clf_name, clf in classifiers.items():\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results[clf_name] = accuracy\n",
        "\n",
        "# Print results\n",
        "output_file = \"classifier_results.txt\"\n",
        "with open(output_file, 'w') as file:\n",
        "    for clf_name, accuracy in results.items():\n",
        "        file.write(f\"{clf_name}: Accuracy - {accuracy}\\n\")\n",
        "\n",
        "# Print success message\n",
        "print(f\"Results saved to {output_file}\")\n",
        "\n",
        "# Now you can proceed with pattern mining using SPMF library and further evaluation and comparison of algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/car.data\", header=None)\n",
        "df.columns = ['buying', 'maintenance', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
        "\n",
        "# Convert categorical variables to numerical\n",
        "df = pd.get_dummies(df, columns=['buying', 'maintenance', 'doors', 'persons', 'lug_boot', 'safety'])\n",
        "\n",
        "# Convert DataFrame to transaction format\n",
        "transactions = []\n",
        "for _, row in df.iterrows():\n",
        "    transaction = []\n",
        "    for col in df.columns:\n",
        "        if col != 'class':\n",
        "            transaction.append(col + \"_\" + str(row[col]))\n",
        "    transactions.append(transaction)\n",
        "\n",
        "# Write transactions to input file\n",
        "input_file = \"input_transactions.txt\"\n",
        "with open(input_file, 'w') as file:\n",
        "    for transaction in transactions:\n",
        "        file.write(\" \".join(transaction) + \"\\n\")\n",
        "\n",
        "# Print success message\n",
        "print(f\"Input transactions saved to {input_file}\")\n",
        "\n",
        "# Now you can proceed with pattern mining using SPMF library and further evaluation and comparison of algorithms.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpjrPq0Z4sG7",
        "outputId": "b5987d92-d34f-4903-af0c-143b0a239d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input transactions saved to input_transactions.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('car.data', header=None)\n",
        "data.columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
        "\n",
        "# Write dataset to a new file\n",
        "data.to_csv('car_input.csv', index=False)\n",
        "\n",
        "# Handle categorical data\n",
        "le = LabelEncoder()\n",
        "data_encoded = data.apply(le.fit_transform)\n",
        "\n",
        "# Split features and target variable\n",
        "X = data_encoded.drop('class', axis=1)\n",
        "y = data_encoded['class']\n",
        "\n",
        "# Handle unbalanced classes\n",
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Model evaluation\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Further analysis can be added as per your requirement.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUiTzb3SHGe5",
        "outputId": "201ea5c2-0951-4d97-9ca5-cf26bbde1613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       356\n",
            "           1       1.00      1.00      1.00       347\n",
            "           2       1.00      0.99      0.99       385\n",
            "           3       1.00      1.00      1.00       364\n",
            "\n",
            "    accuracy                           1.00      1452\n",
            "   macro avg       1.00      1.00      1.00      1452\n",
            "weighted avg       1.00      1.00      1.00      1452\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, fpgrowth\n",
        "\n",
        "# Step 1: Data Preprocessing\n",
        "# Load dataset\n",
        "data = pd.read_csv('car.data', header=None)\n",
        "data.columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
        "\n",
        "# Convert categorical variables into text format\n",
        "data = data.astype(str)\n",
        "\n",
        "# Convert dataframe to transaction format\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(data.values).transform(data.values)\n",
        "transaction_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "# Step 2: Generating Frequent Patterns\n",
        "# Set minimum support threshold\n",
        "min_support = 0.3\n",
        "\n",
        "# Use Apriori algorithm\n",
        "apriori_patterns = apriori(transaction_df, min_support=min_support, use_colnames=True)\n",
        "\n",
        "# Use FP-Growth algorithm\n",
        "fpgrowth_patterns = fpgrowth(transaction_df, min_support=min_support, use_colnames=True)\n",
        "\n",
        "# Step 3: Comparing Algorithms (Not applicable in this approach)\n",
        "\n",
        "# Step 4: Evaluation and Comparison (Not applicable in this approach)\n",
        "# Display the patterns obtained\n",
        "print(\"Apriori Patterns:\")\n",
        "print(apriori_patterns)\n",
        "print(\"\\nFP-Growth Patterns:\")\n",
        "print(fpgrowth_patterns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXy2teE-HaZU",
        "outputId": "a5ed1d78-8b63-447c-e154-4a715378fe02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apriori Patterns:\n",
            "     support           itemsets\n",
            "0   0.500000                (2)\n",
            "1   0.500000                (4)\n",
            "2   0.333333              (big)\n",
            "3   0.625000             (high)\n",
            "4   0.625000              (low)\n",
            "5   0.750000              (med)\n",
            "6   0.333333             (more)\n",
            "7   0.333333            (small)\n",
            "8   0.700231            (unacc)\n",
            "9   0.437500            (vhigh)\n",
            "10  0.312500          (2, high)\n",
            "11  0.312500           (2, low)\n",
            "12  0.375000           (2, med)\n",
            "13  0.438657         (unacc, 2)\n",
            "14  0.312500          (4, high)\n",
            "15  0.312500           (4, low)\n",
            "16  0.375000           (4, med)\n",
            "17  0.306713         (4, unacc)\n",
            "18  0.333333        (high, low)\n",
            "19  0.430556        (high, med)\n",
            "20  0.408565      (unacc, high)\n",
            "21  0.430556         (med, low)\n",
            "22  0.456019       (unacc, low)\n",
            "23  0.502315       (unacc, med)\n",
            "24  0.354167     (unacc, vhigh)\n",
            "25  0.324074    (unacc, 2, med)\n",
            "26  0.304977  (unacc, med, low)\n",
            "\n",
            "FP-Growth Patterns:\n",
            "     support           itemsets\n",
            "0   0.700231            (unacc)\n",
            "1   0.625000              (low)\n",
            "2   0.500000                (2)\n",
            "3   0.437500            (vhigh)\n",
            "4   0.333333            (small)\n",
            "5   0.750000              (med)\n",
            "6   0.625000             (high)\n",
            "7   0.333333              (big)\n",
            "8   0.500000                (4)\n",
            "9   0.333333             (more)\n",
            "10  0.502315       (unacc, med)\n",
            "11  0.456019       (unacc, low)\n",
            "12  0.430556         (med, low)\n",
            "13  0.304977  (unacc, med, low)\n",
            "14  0.438657         (unacc, 2)\n",
            "15  0.312500           (2, low)\n",
            "16  0.375000           (2, med)\n",
            "17  0.312500          (2, high)\n",
            "18  0.324074    (unacc, 2, med)\n",
            "19  0.354167     (unacc, vhigh)\n",
            "20  0.408565      (unacc, high)\n",
            "21  0.430556        (high, med)\n",
            "22  0.333333        (high, low)\n",
            "23  0.312500           (4, low)\n",
            "24  0.306713         (4, unacc)\n",
            "25  0.375000           (4, med)\n",
            "26  0.312500          (4, high)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('car.data', header=None)\n",
        "data.columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
        "\n",
        "# Feature Engineering: Use the mined patterns or association rules as additional features\n",
        "\n",
        "# Split features and target variable\n",
        "X = data.drop('class', axis=1)\n",
        "y = data['class']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Model Training\n",
        "# Initialize Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report without mined patterns/rules:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Experiment with adding mined patterns/rules as additional features and repeat the training and evaluation process\n",
        "\n",
        "# Optimization: No specific optimization demonstrated in this basic example\n",
        "\n",
        "# Documentation and Presentation: Prepare a detailed report and presentation slides\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "gOfqLtqfLZXa",
        "outputId": "bc6710ce-75e4-4b72-bbec-7ff29d8c583a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: 'med'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-aca9e77d241e>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Train the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mrf_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"numpy.array_api\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2070\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m     def __array_wrap__(\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'med'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, fpgrowth, fpmax\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/car.data', header=None)\n",
        "\n",
        "# Set column names\n",
        "data.columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
        "\n",
        "# Preprocessing\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "for column in data.columns:\n",
        "    data[column] = label_encoder.fit_transform(data[column])\n",
        "\n",
        "# Split data into features and target\n",
        "X = data.drop('class', axis=1)\n",
        "y = data['class']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Generate frequent patterns\n",
        "def generate_patterns(data, algorithm, min_support):\n",
        "    te = TransactionEncoder()\n",
        "    te_ary = te.fit(data).transform(data)\n",
        "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "    if algorithm == 'apriori':\n",
        "        patterns = apriori(df, min_support=min_support, use_colnames=True)\n",
        "    elif algorithm == 'fpgrowth':\n",
        "        patterns = fpgrowth(df, min_support=min_support, use_colnames=True)\n",
        "    elif algorithm == 'fpmax':\n",
        "        patterns = fpmax(df, min_support=min_support, use_colnames=True)\n",
        "    return patterns\n",
        "\n",
        "# Compare algorithms\n",
        "def compare_algorithms(data, min_support):\n",
        "    algorithms = ['apriori', 'fpgrowth', 'fpmax']\n",
        "    for algorithm in algorithms:\n",
        "        print(f\"Algorithm: {algorithm}\")\n",
        "        patterns = generate_patterns(data, algorithm, min_support)\n",
        "        print(patterns)\n",
        "\n",
        "# Perform comparison\n",
        "min_support = 0.2  # You can adjust this threshold\n",
        "compare_algorithms(data.values.tolist(), min_support)\n",
        "\n",
        "# Additional steps: You can further tune parameters and evaluate algorithms using classifiers or clustering techniques.\n",
        "# For example:\n",
        "# - Train classifiers using frequent patterns as features\n",
        "# - Evaluate classifiers' performance\n",
        "# - Tune parameters such as min_support, pattern length, etc.\n",
        "# - Experiment with different classifiers or clustering algorithms\n",
        "# - Use cross-validation for robust evaluation\n",
        "\n",
        "# Example: Training a Random Forest classifier\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TQHpTCLdzj9",
        "outputId": "7bae49a5-e139-4166-bc29-c8718eff83ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Algorithm: apriori\n",
            "     support      itemsets\n",
            "0   0.905671           (0)\n",
            "1   0.875000           (1)\n",
            "2   0.979167           (2)\n",
            "3   0.604167           (3)\n",
            "4   0.784144        (0, 1)\n",
            "5   0.884838        (0, 2)\n",
            "6   0.533565        (0, 3)\n",
            "7   0.854167        (1, 2)\n",
            "8   0.515046        (1, 3)\n",
            "9   0.590278        (2, 3)\n",
            "10  0.763310     (0, 1, 2)\n",
            "11  0.447917     (0, 1, 3)\n",
            "12  0.519676     (0, 2, 3)\n",
            "13  0.501157     (1, 2, 3)\n",
            "14  0.434028  (0, 1, 2, 3)\n",
            "Algorithm: fpgrowth\n",
            "     support      itemsets\n",
            "0   0.979167           (2)\n",
            "1   0.905671           (0)\n",
            "2   0.875000           (1)\n",
            "3   0.604167           (3)\n",
            "4   0.884838        (0, 2)\n",
            "5   0.854167        (1, 2)\n",
            "6   0.784144        (0, 1)\n",
            "7   0.763310     (0, 1, 2)\n",
            "8   0.590278        (2, 3)\n",
            "9   0.533565        (0, 3)\n",
            "10  0.515046        (1, 3)\n",
            "11  0.519676     (0, 2, 3)\n",
            "12  0.501157     (1, 2, 3)\n",
            "13  0.447917     (0, 1, 3)\n",
            "14  0.434028  (0, 1, 2, 3)\n",
            "Algorithm: fpmax\n",
            "    support      itemsets\n",
            "0  0.434028  (0, 1, 2, 3)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.90      0.94        83\n",
            "           1       0.61      1.00      0.76        11\n",
            "           2       1.00      1.00      1.00       235\n",
            "           3       1.00      0.94      0.97        17\n",
            "\n",
            "    accuracy                           0.97       346\n",
            "   macro avg       0.90      0.96      0.92       346\n",
            "weighted avg       0.98      0.97      0.98       346\n",
            "\n"
          ]
        }
      ]
    }
  ]
}