{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "luFIASTQmcWf"
   },
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow\n",
    "!pip install tensorflow==2.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poKWk6yV_-MR"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9AkuK4V_9Ym"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy\n",
    "import tensorflow_datasets as tfds\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy import asarray\n",
    "from scipy.linalg import sqrtm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCZj3A1vsR41"
   },
   "source": [
    "## Utils function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87yttZXtqtj3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_images(images, captions=None):\n",
    "  num_ver = len(images)//5 + 1\n",
    "  num_hor = len(images) // num_ver +1\n",
    "  plt.figure(figsize=(10,10))\n",
    "  for i in range(len(images)):\n",
    "    plt.subplot(num_ver,num_hor, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "  plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3S1yVQQhsUd5"
   },
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VrvgPaX-_lg"
   },
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "shape = [299,299]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziDM28t-q9Kf"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzJvK_O6C6JQ"
   },
   "source": [
    "We are using Celeb_A dataset to measure a quality of images generated with pre-trained ProgressiveGAN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJ3Ub9mG7NZd"
   },
   "outputs": [],
   "source": [
    "!wget -q -O celeba.zip https://seafile.unistra.fr/f/15dc03434ab245d2a960/?dl=1\n",
    "!unzip -q celeba.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrDsbUhNAVEL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "cx=89\n",
    "cy=121\n",
    "center = [128,128]\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_image(file_path):\n",
    "    # Read and decode the image file\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    image = img[cy - 64 : cy + 64, cx - 64 : cx + 64]\n",
    "    image = tf.image.resize(image,(shape[0],shape[1]))\n",
    "    image = int(image)\n",
    "    return image\n",
    "\n",
    "# Load the list_eval_partition.csv file\n",
    "csv_path = \"list_eval_partition.csv\"\n",
    "partition_df = pd.read_csv(csv_path)\n",
    "dataset_root = \"img_align_celeba/img_align_celeba\"\n",
    "train_files = partition_df[partition_df['partition'] == 0]['image_id'].tolist()\n",
    "train_paths = [os.path.join(dataset_root, file) for file in train_files]\n",
    "# Create TensorFlow dataset for the training set\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_paths)\n",
    "train_dataset = train_dataset.shuffle(1000)\n",
    "train_dataset = train_dataset.map(load_and_preprocess_image, num_parallel_calls=4)\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7k8clTtdqwfz"
   },
   "source": [
    "## Load pretrained Inception model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpsvxB8tDDzK"
   },
   "source": [
    "InceptionV3 model is used for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SGCWz2V0QQg"
   },
   "outputs": [],
   "source": [
    "inception = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet',input_shape=(shape[0],shape[1],3),pooling='avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBgQTvtNqzoH"
   },
   "source": [
    "## Load pretrained Progressive GAN model\n",
    "\n",
    "Instead of using your own GAN, here we will use already trained Progressive GAN in order to get intuition what FID score represents.\n",
    "\n",
    "Later, you can load your own model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyoBItMk3Tml"
   },
   "outputs": [],
   "source": [
    "progan = hub.load(\"https://tfhub.dev/google/progan-128/1\").signatures['default']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQDfx8x3smyW"
   },
   "source": [
    "## Visualise datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9x2havyrGfX"
   },
   "source": [
    "### Visualise real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YE3Fm5SyKGZV"
   },
   "outputs": [],
   "source": [
    "iterator= iter(train_dataset)\n",
    "real_img = iterator.get_next()\n",
    "display_images(real_img.numpy()/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71pdFj2lrWoy"
   },
   "source": [
    "###Visualise generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vELZmTIrVop"
   },
   "outputs": [],
   "source": [
    "#TODO:\n",
    "noise = tf.random.normal([batch_size, 512])\n",
    "fake_img = progan(noise)['default']*255\n",
    "fake_img = tf.image.resize(fake_img,shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quNeY6bJlmN0"
   },
   "outputs": [],
   "source": [
    "display_images(fake_img.numpy()/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-7So_Vd2Ezz"
   },
   "source": [
    "## Fréchet Distance\n",
    "\n",
    "\n",
    "**Formula**\n",
    "\n",
    "Based on the paper, \"[The Fréchet distance between multivariate normal distributions](https://core.ac.uk/reader/82269844)\" by Dowson and Landau (1982), the Fréchet distance between two multivariate normal distributions $X$ and $Y$ is:\n",
    "\n",
    "$d(X, Y) = \\Vert\\mu_X-\\mu_Y\\Vert^2 + \\mathrm{Tr}\\left(\\Sigma_X+\\Sigma_Y - 2 \\sqrt{\\Sigma_X \\Sigma_Y}\\right)$\n",
    "\n",
    "where $\\mu_x$ and $\\mu_y$ are means of corresponding distributions, and $\\Sigma_X$ and $\\Sigma_Y$ are c\n",
    "ovariance  matrix\n",
    "\n",
    "For calculating means, covariance  matrixes and traces, you can use **numpy library.**\n",
    "\n",
    "*Read documentation carefully, some default paramters might not be what you want*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-kiC9AF2Hpw"
   },
   "outputs": [],
   "source": [
    "# Function receives numpy arrays of pre-trained network's activations (e.g. real images and fake images activations)\n",
    "# shape of activations is  (n_samples,2048) since we are taking last layer in Inception network\n",
    "# mu1,mu2: the mean of the first/second  activations, shape = (n_features)\n",
    "# sigma1,sigma2: the covariance matrix of the first/second activations, shape= (n_features, n_features)\n",
    "\n",
    "def calculate_fid(act1,act2):\n",
    "\t# calculate mean and covariance statistics over\n",
    "\t#TODO:\n",
    "\tmu1 = act1.mean(axis=0)\n",
    "\tsigma1 = cov(act1, rowvar=False)\n",
    "\tmu2 = act2.mean(axis=0)\n",
    "\tsigma2 =  cov(act2, rowvar=False)\n",
    "\t# calculate sum squared difference between means\n",
    "\tssdiff = numpy.sum((mu1 - mu2)**2.0)\n",
    "\t# calculate sqrt of product between cov\n",
    "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
    "\t# check and correct imaginary numbers from sqrt\n",
    "\tif iscomplexobj(covmean):\n",
    "\t\tcovmean = covmean.real\n",
    "\t# calculate score\n",
    "\tfid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "\treturn fid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOn4494iruKd"
   },
   "source": [
    "### FID score between two sets of real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujXo3Fgok_P1"
   },
   "outputs": [],
   "source": [
    "iterator= iter(train_dataset)\n",
    "n_batches = 500\n",
    "real_feature_list_1 = numpy.zeros((n_batches*batch_size,2048))\n",
    "real_feature_list_2 = numpy.zeros((n_batches*batch_size,2048))\n",
    "for i in tqdm(range(n_batches)):\n",
    "  real_img = iterator.get_next()\n",
    "  real_feat = inception(tf.keras.applications.inception_v3.preprocess_input(tf.cast(real_img,tf.float32)))\n",
    "  real_feature_list_1[i*batch_size:(i+1)*batch_size] = real_feat.numpy()\n",
    "\n",
    "  real_img = iterator.get_next()\n",
    "  real_feat = inception(tf.keras.applications.inception_v3.preprocess_input(tf.cast(real_img,tf.float32)))\n",
    "  real_feature_list_2[i*batch_size:(i+1)*batch_size] = real_feat.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGzlCuKplVsn"
   },
   "outputs": [],
   "source": [
    "fid = calculate_fid(real_feature_list_1, real_feature_list_2)\n",
    "print('FID: %.3f' % fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWiVueF_ld7U"
   },
   "outputs": [],
   "source": [
    "fid = calculate_fid(real_feature_list_1, real_feature_list_1)\n",
    "print('FID: %.3f' % fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmGeykZcsiF0"
   },
   "source": [
    "### FID score between real and generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JekZBIeoM1DS"
   },
   "outputs": [],
   "source": [
    "n_batches = 500\n",
    "real_feature_list = numpy.zeros((n_batches*batch_size,2048))\n",
    "fake_feature_list = numpy.zeros((n_batches*batch_size,2048))\n",
    "for i in tqdm(range(n_batches)):\n",
    "  real_img = iterator.get_next()\n",
    "  real_feat = inception(tf.keras.applications.inception_v3.preprocess_input(tf.cast(real_img,tf.float32)))\n",
    "  real_feature_list[i*batch_size:(i+1)*batch_size] = real_feat.numpy()\n",
    "\n",
    "  noise = tf.random.normal([batch_size, 512])\n",
    "  #TODO..\n",
    "  fake_img = progan(noise)['default']*255\n",
    "  fake_img = tf.image.resize(fake_img,shape)\n",
    "  fake_feat =inception(tf.keras.applications.inception_v3.preprocess_input(tf.cast(fake_img,tf.float32)))\n",
    "  fake_feature_list[i*batch_size:(i+1)*batch_size] = fake_feat.numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a-_X-auRNwV"
   },
   "outputs": [],
   "source": [
    "fid = calculate_fid(real_feature_list, fake_feature_list)\n",
    "print('FID: %.3f' % fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BflsQmS2r8jQ"
   },
   "source": [
    "## Playground\n",
    "\n",
    "\n",
    "\n",
    "1.   Experiment with different sample size (batch_size and n_batches)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
