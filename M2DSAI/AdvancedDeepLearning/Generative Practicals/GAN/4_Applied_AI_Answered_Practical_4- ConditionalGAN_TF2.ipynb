{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kl3qXqeoDb07"
   },
   "source": [
    "***\n",
    "University of Strasbourg \n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLUnjsLdA7O9"
   },
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow -y\n",
    "!pip install tensorflow==2.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6537,
     "status": "ok",
     "timestamp": 1643745690891,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "oChX5IitAyXk",
    "outputId": "8b3d127a-51cf-4ee5-b112-89fb79a99963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: comet_ml in /usr/local/lib/python3.7/dist-packages (3.25.0)\n",
      "Requirement already satisfied: websocket-client>=0.55.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (1.2.3)\n",
      "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (2.23.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from comet_ml) (1.15.0)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (7.352.0)\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (3.0.2)\n",
      "Requirement already satisfied: semantic-version>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (2.8.5)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (1.13.3)\n",
      "Requirement already satisfied: everett[ini]>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (3.0.0)\n",
      "Requirement already satisfied: dulwich>=0.20.6 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (0.20.32)\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (4.3.3)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (0.9.1)\n",
      "Requirement already satisfied: urllib3>=1.24.1 in /usr/local/lib/python3.7/dist-packages (from dulwich>=0.20.6->comet_ml) (1.24.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from dulwich>=0.20.6->comet_ml) (2021.10.8)\n",
      "Requirement already satisfied: configobj in /usr/local/lib/python3.7/dist-packages (from everett[ini]>=1.0.1->comet_ml) (5.0.6)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (21.4.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (3.10.0.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (4.10.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.18.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=3.1.0,>=2.6.0->comet_ml) (3.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->comet_ml) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install comet_ml\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poKWk6yV_-MR"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5047,
     "status": "ok",
     "timestamp": 1643745695922,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "d9AkuK4V_9Ym",
    "outputId": "2ea96b9a-c2c1-424d-b24b-76b5a6d91e35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Couldn't find a Git repository in '/content' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/jecajeca/applied-ai-gan-students/74a0cc26b42346ac86fdc4efd74f2b1b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Comet package for tracking the experiments\n",
    "\n",
    "from comet_ml import Experiment\n",
    "### TODO: Signup for a free account at https://www.comet.ml and paste your API key below\n",
    "exp = Experiment(\n",
    "    api_key=\"ZZUpHIDC4hsMQ3ps3tMOe2qWL\",\n",
    "    project_name='applied-ai-gan-students')\n",
    "### END OF TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2747,
     "status": "ok",
     "timestamp": 1643745698652,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "xnjbpUZ0JAG-"
   },
   "outputs": [],
   "source": [
    "# Import models, layers, optmisers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Reshape, Conv2DTranspose, Conv2D, Flatten, LeakyReLU, ReLU,BatchNormalization, Embedding,Multiply,Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import cifar10, mnist\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import initializers\n",
    "#other packages\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from skimage.transform import resize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_URgg8xexel-"
   },
   "source": [
    "### **Global variables** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1643745698653,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "JekZBIeoM1DS"
   },
   "outputs": [],
   "source": [
    "# Globals variables : \n",
    "noise_input_shape = 100\n",
    "batch_size = 128\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WRDdcycGMD5"
   },
   "source": [
    "### Utils functions (plotting, logging)\n",
    "You can ignore these for the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1643745698654,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "Czc4tKc2GVBy"
   },
   "outputs": [],
   "source": [
    "# Function to plot generated images : \n",
    "'''\n",
    "Arguments : \n",
    "epoch - training epoch\n",
    "generator - generator model from epoch epoch\n",
    "exp - Comet.ml callback\n",
    "examples - number of images to be plotted\n",
    "dim - (rows, cols) of figure in which images will be plotted\n",
    "figsize - size of the figure\n",
    "prefix - prefix for the filename of the saved plot\n",
    "'''\n",
    "#TODO: Take a look how generator produce image\n",
    "def plot_generated_images(epoch, generator, exp, examples=10, dim=(2, 5), figsize=(5, 5), prefix='dataset'):\n",
    "    # Random input vector\n",
    "    noise = tf.random.normal(shape=[examples, noise_input_shape])\n",
    "    labels = tf.reshape(tf.range(start=0, limit=num_classes),[-1,1])\n",
    "    # Generate images\n",
    "    generated_images =(generator([noise,labels],training=False)).numpy()\n",
    "    # Convert images into range [0-255] and convert to uint8 for proper plotting\n",
    "    generated_images = ((generated_images * 127.5) + 127.5).astype('uint8')\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i + 1)\n",
    "        plt.imshow(np.squeeze(generated_images[i]))\n",
    "        plt.gca().set_title(\"Digit: %d\" % labels[i])\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(prefix+'_gan_generated_image %d.png' % epoch)\n",
    "    # Log plotted figure into Comet.ml\n",
    "    exp.log_figure(figure_name='generated_images_step'+str(epoch))\n",
    "    plt.close()\n",
    "\n",
    "def plot_generated_images_random(epoch, generator, exp, examples=100, dim=(10, 10), figsize=(10, 10), prefix='dataset'):\n",
    "    # Random input vector\n",
    "    noise = tf.random.normal(shape=[examples, noise_input_shape])\n",
    "    labels = tf.random.uniform(shape=[examples], minval=0, maxval=num_classes, dtype=tf.int32)\n",
    "    # Generate images\n",
    "    generated_images =(generator([noise,labels],training=False)).numpy()\n",
    "    # Convert images into range [0-255] and convert to uint8 for proper plotting\n",
    "    generated_images = ((generated_images * 127.5) + 127.5).astype('uint8')\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i + 1)\n",
    "        plt.imshow(np.squeeze(generated_images[i]))\n",
    "        plt.gca().set_title(\"Digit: %d\" % labels[i])\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('final_gan_generated_image.png')\n",
    "    # Log plotted figure into Comet.ml\n",
    "    exp.log_figure(figure_name='final_gan_generated_image.png')\n",
    "    plt.close()\n",
    "# Function to plot real images : \n",
    "'''\n",
    "Arguments : \n",
    "X_train - real dataset images (or batch of real dataset images)\n",
    "exp - Comet.ml callback\n",
    "examples - number of images to be plotted\n",
    "dim - (rows, cols) of figure in which images will be plotted\n",
    "figsize - size of the figure\n",
    "prefix - prefix for the filename of the saved plot\n",
    "\n",
    "'''\n",
    "def plot_real_images(X_train,y_train,exp, examples=25, dim=(5, 5), figsize=(5, 5), prefix='dataset'):\n",
    "    print(X_train.shape)\n",
    "    real_images = X_train[:examples]\n",
    "    print(real_images.shape)\n",
    "    real_images = ((real_images.numpy() * 127.5) + 127.5).astype('uint8')\n",
    "    print(real_images.shape)\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(real_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i + 1)\n",
    "        plt.imshow(np.squeeze(real_images[i]))\n",
    "        plt.gca().set_title(\"Digit: %d\" % y_train[i])\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "   # plt.savefig(prefix+'_gan_real_image %d.png')\n",
    "    # Log plotted figure into Comet.ml\n",
    "    exp.log_figure(figure_name='real_images.png')\n",
    "    plt.close()\n",
    "\n",
    "# Comet logging gradinets distributions (vanishing gradients)\n",
    "def get_gradients(gradmap, grads, model):\n",
    "    for grad, param in zip(grads, model.trainable_variables):\n",
    "        gradmap.setdefault(param.name, 0)\n",
    "        gradmap[param.name] += grad\n",
    "\n",
    "    return gradmap\n",
    "\n",
    "def log_histogram(experiment, gradmap, step, prefix=None):\n",
    "    for k, v in gradmap.items():\n",
    "        experiment.log_histogram_3d(v, name=\"%s/%s\" % (prefix, k), step=step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1643745698655,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "p8dxlWYcxdif"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggd3EM5t_0JE"
   },
   "source": [
    "## **GAN model**\n",
    "\n",
    "**Conditional  Generator** : \n",
    "\n",
    "*Input*: Noise (shape=(100,)), label (shape = (1,num_classes)).\n",
    "\n",
    "*Output*: Image (32,32,3) \n",
    " \n",
    "**Conditional Discriminator :**\n",
    " \n",
    "*Input*: Generated image and real image (32,32,3), label added as a channel to input image.\n",
    "\n",
    "*Output*: number $\\in [0,1]$, represents is input real or fake (0 fake, 1 real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idTZyfVHsmsR"
   },
   "source": [
    "### Generator : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1643745698656,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "eN3iElnXtn9_"
   },
   "outputs": [],
   "source": [
    "# Function to create generator\n",
    "def create_generator(noise_input_shape=100,out_channels=3):\n",
    "  # Input noise is a vector of length 100 (for example)\n",
    "  input = Input(shape=(noise_input_shape,))\n",
    "  filters = 256\n",
    "  k_size = 4\n",
    "  g = Reshape((1,1,noise_input_shape))(input)\n",
    "  g = Conv2DTranspose(filters,(k_size,k_size),strides=1)(g)\n",
    "  g = BatchNormalization()(g)\n",
    "  g = LeakyReLU(alpha=0.2)(g)\n",
    " \n",
    "  ### TODO: complete the model (see https://keras.io/layers/convolutional/), all leakyReLu layers to have alpha=0.2, 4x4 convolution, and (2,2) stride\n",
    "  # Upsampling blocks\n",
    "  for i in range(1,4):\n",
    "    f = filters//(2**i)\n",
    "    # Spatial upsampling\n",
    "    g = Conv2DTranspose(f, (k_size, k_size), strides=(2, 2), padding='same')(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "\n",
    "  # N.B. Number of channels (filters) in last layer needs to be 3 (output should be an RGB image), with 3x3 convolution\n",
    "  # Activation function in last layer should be tanh to match real images which are in the range [-1,1]\n",
    "  g = Conv2D(out_channels, (3, 3), activation='tanh', padding='same')(g)\n",
    "  ### END OF TODO\n",
    "  \n",
    "  # Make the model\n",
    "  model = Model(inputs=input, outputs=g)\n",
    "\n",
    "  # Model doesn't need to be compiled because it will be trained in combination with discriminator (new model will be created)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1643745983655,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "5i6lbzy_4hkw"
   },
   "outputs": [],
   "source": [
    "#TODO :\n",
    "def create_conditional_generator(num_classes,noise_input_shape,out_channels=3):\n",
    "  # Random noise vector z\n",
    "  z = Input(shape=(noise_input_shape, ))\n",
    "\n",
    "  # Conditioning label: integer 0-9 specifying the number G should generate\n",
    "  label = Input(shape=(1, ), dtype='int32')\n",
    "\n",
    "  # Label embedding: Could be done in a various ways\n",
    "  # Concatantion with noise can result in generator which igonres noise, producing a single output for each class.\n",
    "  # Using embedding layers is another option to mix noise and class lables (in structured and learnable way)\n",
    "  # ----------------\n",
    "  # Turns labels into dense vectors of size z_dim\n",
    "  # Produces 3D tensor with shape (batch_size, 1, z_dim) \n",
    "  label_embedding = Embedding(num_classes, noise_input_shape, input_length=1)(label)\n",
    "\n",
    "  # Flatten the embedding 3D tensor into 2D tensor with shape (batch_size, z_dim)\n",
    "  label_embedding = Flatten()(label_embedding)\n",
    "\n",
    "  # Element-wise product of the vectors z and the label embeddings\n",
    "  joined_representation =  Multiply()([z, label_embedding]) # TODO: Experiment with Concatanate!\n",
    "\n",
    "  #TODO : create generator\n",
    "  generator = create_generator(noise_input_shape,out_channels)\n",
    "  #TODO create conditional generator (use previously created generator and joined_representation)\n",
    "  conditioned_img = generator(joined_representation)\n",
    "\n",
    "  # make model\n",
    "  model =  Model([z, label], conditioned_img)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 690
    },
    "executionInfo": {
     "elapsed": 2676,
     "status": "ok",
     "timestamp": 1643745988106,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "a77iU1Xiw8Tp",
    "outputId": "4d33c66e-c554-4f50-8d4c-78cbd6551101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1, 100)       1000        ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 100)          0           ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 100)          0           ['input_10[0][0]',               \n",
      "                                                                  'flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " model_4 (Functional)           (None, 32, 32, 3)    1100995     ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,101,995\n",
      "Trainable params: 1,101,035\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f68264f2110>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test generator\n",
    "g = create_conditional_generator(num_classes,noise_input_shape,out_channels=3)\n",
    "g.summary()\n",
    "noise = tf.random.normal([1,noise_input_shape])\n",
    "gen_img = g([noise,1],training=False).numpy()\n",
    "plt.imshow(((gen_img[0,:,:]* 127.5) + 127.5).astype('uint8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IP5lZKus_6_"
   },
   "source": [
    "### Discriminator: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1643745699874,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "zxutqe3Ps_c2"
   },
   "outputs": [],
   "source": [
    "# Function to create discriminator\n",
    "def create_discriminator(input_shape):\n",
    "    # Discriminator input has the same shape as the original dataset image shape (i.e. the same as the generator output)\n",
    "    input = Input(input_shape)\n",
    "    filters = 64\n",
    "    k_size= 3\n",
    "    d = Conv2D(filters, (k_size, k_size),padding='same')(input)\n",
    "    #d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    for i in range(1,4):\n",
    "      f = filters*(2 ** i)\n",
    "      d = Conv2D(f, (k_size, k_size), strides=(2, 2),padding='same')(d)\n",
    "      #d = BatchNormalization()(d)\n",
    "      d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    \n",
    "    d = Flatten()(d)\n",
    "    # Output is one number\n",
    "\n",
    "    d = Dense(1)(d)\n",
    "    ### END OF TODO\n",
    "    \n",
    "    model = Model(inputs = input,outputs=d)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "aborted",
     "timestamp": 1643745699875,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "qMEYkR4Z46GU"
   },
   "outputs": [],
   "source": [
    "def create_conditional_discriminator(input_shape,num_classes):\n",
    "  # Input image\n",
    "  img = Input(shape=input_shape)\n",
    "\n",
    "  # Label for the input image\n",
    "  label = Input(shape=(1, ), dtype='int32')\n",
    "\n",
    "  # Label embedding:\n",
    "  # ----------------\n",
    "  # Turns labels into dense vectors of size z_dim\n",
    "  # Produces 3D tensor with shape (batch_size, 1, input_shape[0]*input_shape[1]*1)\n",
    "  label_embedding = Embedding(num_classes,\n",
    "                              input_shape[0]*input_shape[1],\n",
    "                              input_length=1)(label)\n",
    "\n",
    "  # Flatten the embedding 3D tensor into 2D tensor with shape (batch_size, 28*28*1)\n",
    "  label_embedding = Flatten()(label_embedding)\n",
    "\n",
    "  # Reshape label embeddings to have same dimensions as input images\n",
    "  label_embedding = Reshape((input_shape[0],input_shape[1],1))(label_embedding)\n",
    "\n",
    "  # Concatenate images with their label embeddings\n",
    "  concatenated = Concatenate(axis=-1)([img, label_embedding])\n",
    "  discriminator = create_discriminator((input_shape[0],input_shape[1],input_shape[2]+1))\n",
    "\n",
    "  # Predict disciminator outcome for conncataned tensor\n",
    "  classification = discriminator(concatenated)\n",
    "  # make model \n",
    "  model = Model([img, label], classification)\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "aborted",
     "timestamp": 1643745699877,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "EwLOVR7kzlTi"
   },
   "outputs": [],
   "source": [
    "# Test discriminator\n",
    "d = create_conditional_discriminator((32,32,3),num_classes)\n",
    "d.summary()\n",
    "decision = d([gen_img,np.ones(gen_img.shape[0])])\n",
    "#print(decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e34ZO8lktJzu"
   },
   "source": [
    "### Loss functions and optimisers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "aborted",
     "timestamp": 1643745699879,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "N6zkZRVdGEOF"
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = (real_loss + fake_loss)/2.0\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "aborted",
     "timestamp": 1643745699880,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "RZKf3rH9GSud"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4,beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4,beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZE4Kj5vydjI"
   },
   "source": [
    "#  Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "aborted",
     "timestamp": 1643745699880,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "evAQXs2TtJrU"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset='mnist' #'cifar'\n",
    "if dataset=='mnist':\n",
    "  (x_train, y_train), (_, _) = mnist.load_data() \n",
    "  out_channels = 1\n",
    "  x_train = x_train[...,np.newaxis]\n",
    "elif dataset=='cifar':\n",
    "  (x_train, y_train), (_, _) = cifar10.load_data()\n",
    "  out_channels = 3\n",
    "else:\n",
    "  sys.exit('Unknown dataset')\n",
    "  \n",
    "# Normalise data to the range (-1,1)\n",
    "x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "y_train = y_train.astype(np.int32)\n",
    "\n",
    "# Get shape of images from original dataset (used to specify discriminator input shape)\n",
    "init_shape = x_train.shape[1:]\n",
    "\n",
    "# Training details\n",
    "batch_count = x_train.shape[0] // batch_size # how many iteration per epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AO41Sahyupp"
   },
   "source": [
    "# Prepare models and resize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "aborted",
     "timestamp": 1643745699881,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "r8sz1UD5aQ55"
   },
   "outputs": [],
   "source": [
    "#Make models\n",
    "generator = create_conditional_generator(num_classes,noise_input_shape,out_channels)\n",
    "shape = generator.output_shape[1:]\n",
    "discriminator = create_conditional_discriminator(shape,num_classes)\n",
    "\n",
    "# Log and plot progress (Comet.ml)\n",
    "exp.get_callback('keras').set_model(generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "aborted",
     "timestamp": 1643745699886,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "B0eDXxKr8zG6"
   },
   "outputs": [],
   "source": [
    "def parse_function(img, label):\n",
    "    image = tf.image.resize(img,(shape[0],shape[1]))\n",
    "    return image, label\n",
    "# make trainig set\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "train_dataset = train_dataset.shuffle(x_train.shape[0]) \n",
    "train_dataset = train_dataset.map(parse_function, num_parallel_calls=4)\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4ajfgM5Gl5P"
   },
   "source": [
    "# GAN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pj8cLgaXzJlD"
   },
   "source": [
    "## Training step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "aborted",
     "timestamp": 1643745699886,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "febylNJVGosR"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "@tf.function\n",
    "def train_step(real_images,real_lbls):\n",
    "\n",
    "  \n",
    "  noise =  tf.random.normal([real_images.shape[0], noise_input_shape]) \n",
    "  \n",
    "  with tf.GradientTape() as disc_tape:\n",
    "    generated_images = generator([noise,real_lbls], training=True)\n",
    "    real_output = discriminator([real_images,real_lbls], training=True)\n",
    "    fake_output = discriminator([generated_images,real_lbls], training=True)\n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "  \n",
    "  \n",
    "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "  # Update the weights of the discriminator using the discriminator optimizer\n",
    "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "  noise =  tf.random.normal([real_images.shape[0], noise_input_shape])\n",
    "  gen_lbl = tf.random.uniform((real_images.shape[0],1),minval=0,maxval=num_classes,dtype=tf.int32)\n",
    "  with tf.GradientTape() as gen_tape:\n",
    "      # Generate fake images using the generator\n",
    "      generated_images = generator([noise,gen_lbl], training=True)\n",
    "      # Get the discriminator logits for fake images\n",
    "      fake_output = discriminator([generated_images,gen_lbl], training=True)\n",
    "      # Calculate the generator loss\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "\n",
    "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)  \n",
    "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "  \n",
    "\n",
    "  return gen_loss, disc_loss, gradients_of_generator, gradients_of_discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yH1h0ZBTzaGB"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "aborted",
     "timestamp": 1643745699887,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "S0WRXgCPJkCc"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  real_plt = False\n",
    "  cnt = 0\n",
    "  gradmap_generator = {}\n",
    "  gradmap_discriminator = {}\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    step=0\n",
    "    for (image_batch,lbl_batch) in tqdm(dataset):\n",
    "      step = step+1\n",
    "      if not real_plt:\n",
    "         plot_real_images(image_batch,lbl_batch, exp, prefix='real')\n",
    "         real_plt = True\n",
    "      gen_loss, disc_loss, gradients_of_generator, gradients_of_discriminator= train_step(image_batch,lbl_batch)\n",
    "\n",
    "      \n",
    "      exp.set_step(cnt)\n",
    "      exp.log_metric(\"disc_loss\",disc_loss,step=cnt)\n",
    "      exp.log_metric(\"gen_loss\", gen_loss, step=cnt)\n",
    "      cnt = cnt+1\n",
    "      if cnt % 100 == 0 :\n",
    "        plot_generated_images(cnt, generator, exp, prefix='cifar_' +str(cnt))\n",
    "     # scale gradients\n",
    "    for k, v in gradmap_generator.items():\n",
    "      gradmap_generator[k] = v / step\n",
    "    for k, v in gradmap_discriminator.items():\n",
    "      gradmap_discriminator[k] = v / step\n",
    "\n",
    "    log_histogram(exp, gradmap_generator,cnt, prefix=\"gradient_generator\")\n",
    "    log_histogram(exp, gradmap_discriminator,cnt, prefix=\"gradient_discriminator\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ac4hKGrU0365"
   },
   "source": [
    "##Run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 32,
     "status": "aborted",
     "timestamp": 1643745699887,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "hD6_QSzEJS0P"
   },
   "outputs": [],
   "source": [
    " train(train_dataset, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "aborted",
     "timestamp": 1643745699888,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "jbtsinRAFT4v"
   },
   "outputs": [],
   "source": [
    "plot_generated_images_random(10000,generator,exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 503,
     "status": "aborted",
     "timestamp": 1643745700358,
     "user": {
      "displayName": "Jelica Vasiljevic",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "06732247105640761567"
     },
     "user_tz": -60
    },
    "id": "KnGOBybys6VU"
   },
   "outputs": [],
   "source": [
    "exp.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6N3_ujS8i43"
   },
   "source": [
    "# GAN playground\n",
    "\n",
    "Try to change: \n",
    "\n",
    "\n",
    "1.   Loss function used for training\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "4_Applied_AI_Answered_Practical_4- ConditionalGAN_TF2",
   "provenance": [
    {
     "file_id": "1eV5JU6fYRfpLMwuT23qpwO6IfwhKeHzD",
     "timestamp": 1606919632033
    },
    {
     "file_id": "1xBlBvKts6iF6c9UbNtrgku3v4jjAMMpm",
     "timestamp": 1606917818691
    },
    {
     "file_id": "1apHBYf_trxo-vsVrw5W3VJj28VSUBJ_F",
     "timestamp": 1606822232337
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
