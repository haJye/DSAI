{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLUnjsLdA7O9"
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow\n",
    "!pip install tensorflow==2.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oChX5IitAyXk",
    "outputId": "0904e5f2-ac1c-4a8b-b1b9-948c14e6fe02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: comet_ml in /usr/local/lib/python3.7/dist-packages (3.25.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (2.23.0)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (1.13.3)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (0.9.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from comet_ml) (1.15.0)\n",
      "Requirement already satisfied: dulwich>=0.20.6 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (0.20.32)\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (3.0.2)\n",
      "Requirement already satisfied: everett[ini]>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (3.0.0)\n",
      "Requirement already satisfied: semantic-version>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (2.8.5)\n",
      "Requirement already satisfied: websocket-client>=0.55.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (1.2.3)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (7.352.0)\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (4.3.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from dulwich>=0.20.6->comet_ml) (2021.10.8)\n",
      "Requirement already satisfied: urllib3>=1.24.1 in /usr/local/lib/python3.7/dist-packages (from dulwich>=0.20.6->comet_ml) (1.24.3)\n",
      "Requirement already satisfied: configobj in /usr/local/lib/python3.7/dist-packages (from everett[ini]>=1.0.1->comet_ml) (5.0.6)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.18.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (4.10.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (3.10.0.2)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (5.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (21.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=3.1.0,>=2.6.0->comet_ml) (3.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->comet_ml) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install comet_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poKWk6yV_-MR"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvxCMSK65g-H"
   },
   "source": [
    "TODO: Signup for a free account at https://www.comet.ml and paste your API key below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9AkuK4V_9Ym",
    "outputId": "d87a176c-a1dc-4772-8cfd-6671971de37b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Couldn't find a Git repository in '/content' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/mihailoobrenovic/gans-exercise/3e75a8e6e2f44f6cbc41095370be1a91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Comet package for tracking the experiments\n",
    "\n",
    "from comet_ml import Experiment\n",
    "### TODO: Signup for a free account at https://www.comet.ml and paste your API key below\n",
    "exp = Experiment(\n",
    "    api_key=\"...\",\n",
    "    project_name='applied-ai-gan-students')\n",
    "### END OF TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnjbpUZ0JAG-"
   },
   "outputs": [],
   "source": [
    "# Import models, layers, optmisers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Reshape, Conv2DTranspose, Conv2D, Flatten, LeakyReLU, ReLU,BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import cifar10, mnist\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import initializers\n",
    "#other packages\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from skimage.transform import resize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_URgg8xexel-"
   },
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8dxlWYcxdif"
   },
   "outputs": [],
   "source": [
    "# Globals variables : \n",
    "noise_input_shape = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WRDdcycGMD5"
   },
   "source": [
    "## Utils functions (plotting, logging)\n",
    "Take a look at TODOs ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Czc4tKc2GVBy"
   },
   "outputs": [],
   "source": [
    "# Function to plot generated images : \n",
    "'''\n",
    "Arguments : \n",
    "epoch - training epoch\n",
    "generator - generator model from epoch epoch\n",
    "exp - Comet.ml callback\n",
    "examples - number of images to be plotted\n",
    "dim - (rows, cols) of figure in which images will be plotted\n",
    "figsize - size of the figure\n",
    "prefix - prefix for the filename of the saved plot\n",
    "'''\n",
    "def plot_generated_images(epoch, generator, exp, examples=25, dim=(5, 5), figsize=(5, 5), prefix='dataset'):\n",
    "    # TODO: Random input vector (shape=(num_examples,noise_input_shape))\n",
    "    noise = tf.random.normal([examples, noise_input_shape])\n",
    "    # TODO: Generate images from created noise vector\n",
    "    generated_images = generator.predict(noise)\n",
    "    # Convert images into range [0-255] and convert to uint8 for proper plotting\n",
    "    generated_images = ((generated_images * 127.5) + 127.5).astype('uint8')\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i + 1)\n",
    "        plt.imshow(np.squeeze(generated_images[i]))\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(prefix+'_gan_generated_image %d.png' % epoch)\n",
    "    # Log plotted figure into Comet.ml\n",
    "    exp.log_figure(figure_name='generated_images_ep'+str(epoch))\n",
    "    plt.close()\n",
    "\n",
    "# Function to plot real images : \n",
    "'''\n",
    "Arguments : \n",
    "X_train - real dataset images (or batch of real dataset images)\n",
    "exp - Comet.ml callback\n",
    "examples - number of images to be plotted\n",
    "dim - (rows, cols) of figure in which images will be plotted\n",
    "figsize - size of the figure\n",
    "prefix - prefix for the filename of the saved plot\n",
    "\n",
    "'''\n",
    "def plot_real_images(X_train, exp, examples=25, dim=(5, 5), figsize=(5, 5), prefix='dataset'):\n",
    "    print(X_train.shape)\n",
    "    real_images = X_train[:examples]\n",
    "    print(real_images.shape)\n",
    "    real_images = ((real_images.numpy() * 127.5) + 127.5).astype('uint8')\n",
    "    print(real_images.shape)\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(real_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i + 1)\n",
    "        plt.imshow(np.squeeze(real_images[i]))\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(prefix+'_gan_real_image %d.png')\n",
    "    # Log plotted figure into Comet.ml\n",
    "    exp.log_figure(figure_name='real_images.png')\n",
    "    plt.close()\n",
    "\n",
    "# Comet logging gradinets distributions (vanishing gradients)\n",
    "def get_gradients(gradmap, grads, model):\n",
    "    for grad, param in zip(grads, model.trainable_variables):\n",
    "        gradmap.setdefault(param.name, 0)\n",
    "        gradmap[param.name] += grad\n",
    "\n",
    "    return gradmap\n",
    "def log_histogram(experiment, gradmap, step, prefix=None):\n",
    "    for k, v in gradmap.items():\n",
    "        experiment.log_histogram_3d(v, name=\"%s/%s\" % (prefix, k), step=step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmcCofYMuEVs"
   },
   "source": [
    "##  Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JfPOSsLGuSrq"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset='mnist' #'cifar'\n",
    "if dataset=='mnist':\n",
    "  (x_train, y_train), (_, _) = mnist.load_data() \n",
    "  out_channels = 1\n",
    "  x_train = x_train[...,np.newaxis]\n",
    "elif dataset=='cifar':\n",
    "  (x_train, y_train), (_, _) = cifar10.load_data()\n",
    "  out_channels = 3\n",
    "else:\n",
    "  sys.exit('Unknown dataset')\n",
    "  \n",
    "# Normalise data to the range (-1,1)\n",
    "x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "y_train = y_train.astype(np.int32)\n",
    "\n",
    "# Training details\n",
    "batch_count = x_train.shape[0] // batch_size # how many iteration per epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-efEUF9vPnb"
   },
   "outputs": [],
   "source": [
    "# If dataset is MNIST, we will resize the images to 32x32 size because 32 is more convenient being a power of 2.\n",
    "# CIFAR images are already of size 32x32, resizing will change nothing\n",
    "resize_shape = (32, 32, out_channels)\n",
    "\n",
    "def parse_function(img):\n",
    "    image = tf.image.resize(img,(resize_shape[0],resize_shape[1]))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ni5Q7Z7HuWEO"
   },
   "outputs": [],
   "source": [
    "# make trainig set\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(x_train.shape[0]) \n",
    "train_dataset = train_dataset.map(parse_function, num_parallel_calls=4)\n",
    "train_dataset = train_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggd3EM5t_0JE"
   },
   "source": [
    "## **GAN model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idTZyfVHsmsR"
   },
   "source": [
    "### Generator : \n",
    "\n",
    "Generator transforms input noise to image. The input noise is reshaped to 3D tensor and upsampling is done via TransposeConvolution (take a look at https://keras.io/api/layers/convolution_layers/convolution2d_transpose/).\n",
    "\n",
    "Complete the generator to have the following architecture: \n",
    "\n",
    "<img src=\"https://seafile.unistra.fr/f/92b1617dc1724e39abd4/?dl=1\"/>\n",
    "\n",
    "\n",
    "*Input*: Noise (shape=(noise_input_shape,)).\n",
    "\n",
    "*Output*: Image (32,32,1) for MNIST or (32,32,3) for CIFAR.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eN3iElnXtn9_"
   },
   "outputs": [],
   "source": [
    "# TODO: Function to create generator\n",
    "def create_generator(noise_input_shape=100,out_channels=3):\n",
    "  # Input noise is a vector of length 100 (for example)\n",
    "  input = Input(shape=(noise_input_shape,))\n",
    "  filters = 256\n",
    "  k_size = 4\n",
    "  g = Reshape((1,1,noise_input_shape))(input)\n",
    "  g = Conv2DTranspose(filters,(k_size,k_size),strides=1,kernel_initializer=initializers.RandomNormal(stddev=0.02),bias_initializer=initializers.RandomNormal(stddev=0.02))(g)\n",
    "  g = LeakyReLU(alpha=0.2)(g)\n",
    " \n",
    "  ### TODO: complete the model (see https://keras.io/layers/convolutional/), all leakyReLu layers to have alpha=0.2, 4x4 convolution, and (2,2) stride\n",
    "  # Upsampling blocks\n",
    "  for i in range(1,4):\n",
    "    f = filters//(2**i)\n",
    "    # Spatial upsampling\n",
    "    g = Conv2DTranspose(f, (k_size, k_size), strides=(2, 2), padding='same')(g)\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "\n",
    "  # N.B. Number of channels (filters) in the last layer needs to be 1 for MNIST (output should be grayscale image),\n",
    "  # or 3 for CIFAR (output should be an RGB image), with 3x3 convolution\n",
    "  # Activation function in last layer should be tanh to match real images which are in the range [-1,1]\n",
    "  g = Conv2D(out_channels, (3, 3), activation='tanh', padding='same')(g)\n",
    "  ### END OF TODO\n",
    "  \n",
    "  # Make the model\n",
    "  model = Model(inputs=input, outputs=g)\n",
    "\n",
    "  # Model doesn't need to be compiled because it will be trained in custom training \n",
    "  # loop in combination with discriminator (new model will be created)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "a77iU1Xiw8Tp",
    "outputId": "04bd7c46-814d-4c31-97de-fbb47fe8ab77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 1, 100)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 4, 4, 256)        409856    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 8, 8, 128)        524416    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 16, 16, 64)       131136    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 32, 32, 32)       32800     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 32, 32, 1)         289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,098,497\n",
      "Trainable params: 1,098,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7b304d1450>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test generator\n",
    "g = create_generator(out_channels=out_channels)\n",
    "g.summary()\n",
    "noise = tf.random.normal([1,noise_input_shape])\n",
    "gen_img = g(noise,training=False).numpy()\n",
    "plot_img = np.squeeze(gen_img)\n",
    "plt.imshow(((plot_img * 127.5) + 127.5).astype('uint8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IP5lZKus_6_"
   },
   "source": [
    "### Discriminator: \n",
    "\n",
    "Discriminator distinguishing between real and fake samples:\n",
    "It takes an image as an input and produce the probability of a given image being real.\n",
    "This can be acheived  by using sigmoid activation function at the last layer. For computational stability, use **linear activation** in last layer (no activation) and **sigmoid will be added during loss calculation**.\n",
    "\n",
    "\n",
    "Complete the discriminator to have the following architecture: \n",
    "\n",
    "<img src=\"https://seafile.unistra.fr/f/2739577c4dde45cca672/?dl=1\"/>\n",
    "\n",
    "\n",
    "*Input*: Image (32,32,1) for MNIST or (32,32,3) for CIFAR.\n",
    "\n",
    "*Output*: logit of binary classification (real or fake), shape=(1,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxutqe3Ps_c2"
   },
   "outputs": [],
   "source": [
    "# TODO: Function to create discriminator\n",
    "def create_discriminator(input_shape):\n",
    "    # Discriminator input has the same shape as the generator output\n",
    "    input = Input(input_shape)\n",
    "    filters = 64\n",
    "    k_size= 3\n",
    "    d = Conv2D(filters, (k_size, k_size),padding='same')(input)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    ### TODO: complete the model - all convolutional layers to have a stride of (2,2), all leakyReLu layers to have alpha=0.2\n",
    "    for i in range(1,4):\n",
    "      f = filters*(2 ** i)\n",
    "      d = Conv2D(f, (k_size, k_size), strides=(2, 2),padding='same')(d)\n",
    "      d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    \n",
    "    d = Flatten()(d)\n",
    "    # Output is one number\n",
    "    # Ideal output should be close to 1 for images from real dataset, close to 0 for generated images\n",
    "    d = Dense(1)(d)\n",
    "    ### END OF TODO\n",
    "    \n",
    "    model = Model(inputs = input,outputs=d)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwLOVR7kzlTi",
    "outputId": "8396efb4-fd61-4c0f-bc8d-984572c0025a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)        640       \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 8193      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,558,017\n",
      "Trainable params: 1,558,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "tf.Tensor([[2.1817388e-05]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Test discriminator\n",
    "d = create_discriminator(resize_shape)\n",
    "d.summary()\n",
    "decision = d(gen_img)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "NeODLUTsOXYH",
    "outputId": "246523ba-3c76-4bb7-bb85-fe3610db552d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.0004838]], shape=(1, 1), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7aca6c2a10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this multiple times to see differences\n",
    "noise = tf.random.normal([1,noise_input_shape])\n",
    "gen_img = g(noise,training=False).numpy()\n",
    "decision = d(gen_img)\n",
    "print(decision)\n",
    "plot_img = np.squeeze(gen_img)\n",
    "plt.imshow(((plot_img* 127.5) + 127.5).astype('uint8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e34ZO8lktJzu"
   },
   "source": [
    "### Loss functions and optimisers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6zkZRVdGEOF"
   },
   "outputs": [],
   "source": [
    "#TODO: Create binary cross entropy loss function (use parameter from_logits = True if activation is linear in disciminator).\n",
    "loss_function = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "#TODO: (hint: disciminator should predict one for real data)\n",
    "def discriminator_loss_real_data(disc_pred_for_real_data):\n",
    "    real_loss = loss_function(tf.ones_like(disc_pred_for_real_data), disc_pred_for_real_data)\n",
    "    return real_loss\n",
    "#TODO: (hint: disciminator should  predict zero for real data)\n",
    "def discriminator_loss_fake_data(disc_pred_for_generated_data):\n",
    "    fake_loss = loss_function(tf.zeros_like(disc_pred_for_generated_data), disc_pred_for_generated_data)\n",
    "    return fake_loss\n",
    "\n",
    "#TODO: (hint: generator wants that discriminator's predictions are close to one for generated data)\n",
    "def generator_loss(disc_pred_for_generated_data):\n",
    "    gen_loss =  loss_function(tf.ones_like(disc_pred_for_generated_data), disc_pred_for_generated_data)\n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZKf3rH9GSud"
   },
   "outputs": [],
   "source": [
    "#Optimisers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4,beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4,beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AO41Sahyupp"
   },
   "source": [
    "# Prepare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8sz1UD5aQ55"
   },
   "outputs": [],
   "source": [
    "#TODO: Make models\n",
    "generator = create_generator(noise_input_shape=noise_input_shape,out_channels=out_channels)\n",
    "shape = generator.output_shape[1:]\n",
    "discriminator = create_discriminator(shape)\n",
    "\n",
    "# Log and plot progress (Comet.ml)\n",
    "exp.get_callback('keras').set_model(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4ajfgM5Gl5P"
   },
   "source": [
    "# GAN training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pj8cLgaXzJlD"
   },
   "source": [
    "## Training step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "febylNJVGosR"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_images):\n",
    "  ## Train discriminator\n",
    "  #0. Make a noise vector\n",
    "  noise = tf.random.normal([real_images.shape[0], noise_input_shape])\n",
    "  with tf.GradientTape() as disc_tape:\n",
    "    # 1. generator generates an image from noise\n",
    "    generated_images = generator(noise, training=True)\n",
    "    # 2. discriminator prediction for real images\n",
    "    real_output = discriminator(real_images, training=True)\n",
    "    # 3. discriminator prediction for fake images\n",
    "    fake_output = discriminator(generated_images, training=True)\n",
    "    # 4. discriminator loss\n",
    "    disc_loss = (discriminator_loss_real_data(real_output)+discriminator_loss_fake_data(fake_output))/2\n",
    "  \n",
    "  #get disciminator's gradients\n",
    "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "  # Update the weights of the discriminator using the discriminator optimizer\n",
    "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "  ## Train generator\n",
    "  #0. Make a noise vector\n",
    "  noise = tf.random.normal([real_images.shape[0], noise_input_shape])\n",
    "  with tf.GradientTape() as gen_tape:\n",
    "      # 1. Generate fake images using the generator\n",
    "      generated_images =  generator(noise, training=True)\n",
    "      # 2. Get the discriminator prediction for fake images\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "      # 3. Calculate the generator loss\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "\n",
    "  #TODO: Update generator\n",
    "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)  \n",
    "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "  \n",
    "\n",
    "  return gen_loss, disc_loss, gradients_of_generator, gradients_of_discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yH1h0ZBTzaGB"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0WRXgCPJkCc"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(dataset, epochs):\n",
    "  real_plt = False\n",
    "  cnt = 0\n",
    "  gradmap_generator = {}\n",
    "  gradmap_discriminator = {}\n",
    "\n",
    "  for epoch in tqdm(range(epochs)):\n",
    "    step=0\n",
    "    for image_batch in dataset:\n",
    "      step = step+1\n",
    "      if not real_plt:\n",
    "         plot_real_images(image_batch, exp, prefix='real')\n",
    "         real_plt = True\n",
    "      \n",
    "      gen_loss, disc_loss, gradients_of_generator, gradients_of_discriminator= train_step(image_batch)\n",
    "      \n",
    "      # Logs for comet\n",
    "      gradmap_generator = get_gradients(gradmap_generator, gradients_of_generator, generator)\n",
    "      gradmap_discriminator = get_gradients(gradmap_discriminator, gradients_of_discriminator, discriminator)\n",
    "      \n",
    "      exp.set_step(cnt)\n",
    "      exp.log_metric(\"disc_loss\",disc_loss,step=cnt)\n",
    "      exp.log_metric(\"gen_loss\", gen_loss, step=cnt)\n",
    "      cnt = cnt+1\n",
    "     # scale gradients\n",
    "    for k, v in gradmap_generator.items():\n",
    "      gradmap_generator[k] = v / step\n",
    "    for k, v in gradmap_discriminator.items():\n",
    "      gradmap_discriminator[k] = v / step\n",
    "\n",
    "    plot_generated_images(epoch, generator, exp, prefix='cifar')\n",
    "    log_histogram(exp, gradmap_generator,cnt, prefix=\"gradient_generator\")\n",
    "    log_histogram(exp, gradmap_discriminator,cnt, prefix=\"gradient_discriminator\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ac4hKGrU0365"
   },
   "source": [
    "##Run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hD6_QSzEJS0P",
    "outputId": "3abfce0e-7554-4478-9795-5031639b8956"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32, 32, 1)\n",
      "(25, 32, 32, 1)\n",
      "(25, 32, 32, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [08:22<00:00, 50.28s/it]\n"
     ]
    }
   ],
   "source": [
    "train(train_dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KnGOBybys6VU",
    "outputId": "a133c9c1-1ca5-4a28-98e9-c0d896593883"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/mihailoobrenovic/gans-exercise/3e75a8e6e2f44f6cbc41095370be1a91\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     disc_loss [4690] : (0.021559037268161774, 2.4108169078826904)\n",
      "COMET INFO:     gen_loss [4690]  : (0.035276371985673904, 7.212374687194824)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     notebook_url : https://colab.research.google.com/notebook#fileId=1e9V3MnkFjjuNv-e8mxBzIkVaLvigSnTS\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Adam_amsgrad       : False\n",
      "COMET INFO:     Adam_beta_1        : 0.5\n",
      "COMET INFO:     Adam_beta_2        : 0.999\n",
      "COMET INFO:     Adam_decay         : 0.0\n",
      "COMET INFO:     Adam_epsilon       : 1e-07\n",
      "COMET INFO:     Adam_learning_rate : 0.0002\n",
      "COMET INFO:     Optimizer          : Adam\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details : 1\n",
      "COMET INFO:     figures             : 11\n",
      "COMET INFO:     filename            : 1\n",
      "COMET INFO:     histogram3d         : 200\n",
      "COMET INFO:     installed packages  : 1\n",
      "COMET INFO:     notebook            : 2\n",
      "COMET INFO:     os packages         : 1\n",
      "COMET INFO:     source_code         : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
     ]
    }
   ],
   "source": [
    "exp.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6N3_ujS8i43"
   },
   "source": [
    "# GAN playground\n",
    "\n",
    "1. Try longer training\n",
    "1. Train GAN with CIFAR dataset\n",
    "1.  Change the learning rate of the Generator/Discriminator, try different optimisers\n",
    "2.  Change Input noise shape\n",
    "3.   Modify discriminator/generator architecture (adding BatchNormalisarion, deeper generator/discriminator) \n",
    "4.   Increase the number of discriminator training iterations relative to the generator per epoch, e.g. discriminator is trained 2 times and generator 1 time per epoch\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "R6N3_ujS8i43"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
